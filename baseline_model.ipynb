{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the baseline random classifier model using SKLearn's DummyClassifier. The performance of this model will act as a baseline for the performance of the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoding mapping:\n",
      "{'negative': 0, 'positive': 1}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv(\"IMDB Dataset Processed Lemma test.csv\")\n",
    "\n",
    "###Representing the textual data in a suitable model (i.e. Bag of Words, TF-IDF Vectors)\n",
    "\n",
    "#Represent the text data using Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(df['cleaned_review'])\n",
    "\n",
    "#Alternatively, represent the text data using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_review'])\n",
    "\n",
    "\n",
    "###Splitting the data into the training and test sets. Ensure that the train and test datasets are balanced by using stratify on the sentiments data\n",
    "\n",
    "#Labels (i.e. Sentiment)\n",
    "y = df['sentiment']\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "#Get the mapping of the numeric labels to the original labels\n",
    "label_mapping = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
    "print(\"Label encoding mapping:\")\n",
    "print(label_mapping)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_encoded, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train confusion matrix is:\n",
      "[[ 9940  9818]\n",
      " [ 9845 10062]]\n",
      "Test confusion matrix is:\n",
      "[[2455 2485]\n",
      " [2529 2448]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.50      0.49      4940\n",
      "           1       0.50      0.49      0.49      4977\n",
      "\n",
      "    accuracy                           0.49      9917\n",
      "   macro avg       0.49      0.49      0.49      9917\n",
      "weighted avg       0.49      0.49      0.49      9917\n",
      "\n",
      "Train accuracy score:  0.5042732887936467\n",
      "Test accuracy score:  0.49440354946052234\n",
      "Train ROC-AUC score: 0.5\n",
      "Test ROC-AUC score: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, make_scorer, f1_score, roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy='uniform', random_state=42)\n",
    "\n",
    "# Fit the baseline classifier on the training data\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the classes and the probabilities using the best model\n",
    "predicted_class = dummy_clf.predict(X_test)\n",
    "predicted_class_train = dummy_clf.predict(X_train)\n",
    "test_probs = dummy_clf.predict_proba(X_test)\n",
    "train_probs = dummy_clf.predict_proba(X_train)\n",
    "\n",
    "# Calculate and print the performance metrics\n",
    "print('Train confusion matrix is:')\n",
    "print(confusion_matrix(y_train, predicted_class_train))\n",
    "print('Test confusion matrix is:')\n",
    "print(confusion_matrix(y_test, predicted_class))\n",
    "print(classification_report(y_test, predicted_class))\n",
    "\n",
    "# Calculate train and test accuracy\n",
    "train_accuracy = accuracy_score(y_train, predicted_class_train)\n",
    "test_accuracy = accuracy_score(y_test, predicted_class)\n",
    "print(\"Train accuracy score: \", train_accuracy)\n",
    "print(\"Test accuracy score: \", test_accuracy)\n",
    "\n",
    "# Calculate and print the AUC-ROC score\n",
    "train_auc = roc_auc_score(y_train, train_probs[:, 1], multi_class = 'ovr')\n",
    "test_auc = roc_auc_score(y_test, test_probs[:, 1], multi_class='ovr')\n",
    "print(\"Train ROC-AUC score:\", train_auc)\n",
    "print(\"Test ROC-AUC score:\", test_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
