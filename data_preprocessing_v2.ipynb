{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the data preprocessing notebook.\n",
    "\n",
    "The steps taken to preprocess the data will include:\n",
    "1. Checking the dataset for NA values\n",
    "2. Checking the dataset for duplicate records\n",
    "3. Tokenizing the textual data\n",
    "4. Removing irrelevant data (i.e. HTML tags and special characters)\n",
    "5. Removing stopwords (i.e. Common words like \"the\", \"a\")\n",
    "6. Lemmatization - Representing each token in its base word\n",
    "7. Correcting spelling errors and standardising words to lowercase\n",
    "8. Representing the textual data in a suitable model (i.e. Bag of Words, TF-IDF Vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download and import the libraries required to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\arona\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\arona\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\arona\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\arona\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\arona\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arona\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: textblob in c:\\users\\arona\\anaconda3\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\arona\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\arona\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\arona\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\arona\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\arona\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arona\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\arona\\anaconda3\\lib\\site-packages (8.0.4)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (6.29.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\arona\\anaconda3\\lib\\site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\arona\\anaconda3\\lib\\site-packages (from ipywidgets) (4.0.5)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\arona\\anaconda3\\lib\\site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.8)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.4)\n",
      "Requirement already satisfied: decorator in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\arona\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arona\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (306)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\arona\\appdata\\roaming\\python\\python311\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\arona\\anaconda3\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: contractions in c:\\users\\arona\\anaconda3\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\arona\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\arona\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\arona\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arona\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arona\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install textblob\n",
    "!pip install ipywidgets\n",
    "!pip install contractions\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "#Download stopwords and punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the CSV data of the IMDB Movie Reviews. Here we also explore the dataset in terms of its length, columns and balance.\n",
    "- We observe that the dataset contains 50,000 records.\n",
    "- The dataset contains 2 columns: review and sentiment. The \"review\" column contains records of the textual data (i.e. str) of the IMDB Movie Reviews. The \"sentiment\" column contains the corresponding label of the review. It tells us whether the review has a positive or negative sentiment. There are only 2 possible sentiments: positive and negative.\n",
    "- The dataset is also balanced with an equal number of positive and negative sentiment reviews. There are 25,000 records for each sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "\n",
      "                                                   review sentiment\n",
      "count                                               50000     50000\n",
      "unique                                              49582         2\n",
      "top     Loved today's show!!! It was a variety and not...  positive\n",
      "freq                                                    5     25000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "print(df.head())\n",
    "print()\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The length of the dataframe is 50000.\n",
      "\n",
      "Index(['review', 'sentiment'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Length\n",
    "print(f'\\nThe length of the dataframe is {len(df)}.')\n",
    "print()\n",
    "\n",
    "#Columns\n",
    "print(df.columns)\n",
    "\n",
    "#Balance\n",
    "df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the dataset for NA values \n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    49582\n",
      "True       418\n",
      "Name: review, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Checking the dataset for duplicate records\n",
    "print(df[\"review\"].duplicated().value_counts())\n",
    "\n",
    "#Removing the 418 duplicate records and reset the index for the deduplicated dataset\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    24884\n",
       "negative    24698\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check again that the data is balanced. The dataset is still balanced with 24884 positive sentiment records and 24698 negative sentiment records.\n",
    "df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the preprocessing functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removal of irrelevant data\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removal of Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correcting spelling errors and standardising words to lowercase\n",
    "def correct_spelling(text):\n",
    "    blob = TextBlob(text)\n",
    "    return str(blob.correct()).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\arona\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Define contraction expansion function\n",
    "def expand_contractions(text):\n",
    "    \"\"\"Expand contractions in the text.\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# Define function to handle negations by combining negation words with the following word\n",
    "def handle_negations(text):\n",
    "    \"\"\"Convert negated phrases like 'not good' into 'not_good' to preserve sentiment context.\"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    negations = {\"not\", \"no\", \"never\", \"none\", \"nobody\", \"nothing\", \"nowhere\", \"neither\", \"nor\", \"can't\", \"won't\", \"don't\"}\n",
    "    processed_tokens = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if tokens[i] in negations and i + 1 < len(tokens):\n",
    "            # Combine negation with the next token\n",
    "            processed_tokens.append(f\"{tokens[i]}_{tokens[i + 1]}\")\n",
    "            i += 2  # Skip the next word since it's already combined\n",
    "        else:\n",
    "            processed_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\arona\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\arona\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "100%|██████████| 49582/49582 [09:09<00:00, 90.21it/s] \n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from tqdm import tqdm\n",
    "# Download POS tagger and WordNet if not already downloaded\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Map NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Define function for POS-based filtering\n",
    "def pos_filtering(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    # Filter to include only adjectives, adverbs, nouns, and verbs\n",
    "    filtered_tokens = [word for word, tag in tagged_tokens if get_wordnet_pos(tag) in {wordnet.ADJ, wordnet.ADV, wordnet.NOUN, wordnet.VERB}]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "tqdm.pandas()  # Enable tqdm in notebook mode\n",
    "df['filtered_review'] = df['review'].progress_apply(pos_filtering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49582/49582 [12:15:35<00:00,  1.12it/s]     \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def parallel_preprocess_text(text):\n",
    "    text = expand_contractions(text)  # Expand contractions\n",
    "    text = handle_negations(text)  # Handle negations\n",
    "    text = clean_text(text)  # Clean the text\n",
    "    text = correct_spelling(text)  # Correct spelling and convert to lowercase\n",
    "    tokens = tokenize(text)  # Tokenize\n",
    "    tokens = remove_stopwords(tokens)  # Remove stopwords\n",
    "    tokens = lemmatize_words(tokens)  # Apply lemmatization\n",
    "    return ' '.join(tokens)  # Return processed text\n",
    "\n",
    "# Apply preprocessing to the entire dataset using df['filtered_review'] as input\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Wrap tqdm in a function to work with joblib\n",
    "tqdm.pandas()\n",
    "\n",
    "# Track progress with Parallel and delayed\n",
    "# 'n_jobs=-1' uses all available CPU cores, and 'total=len(df)' lets tqdm know the total iterations \n",
    "df['cleaned_review'] = Parallel(n_jobs=-1)(\n",
    "    delayed(parallel_preprocess_text)(text) for text in tqdm(df['filtered_review'], total=len(df))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the preprocessing functions to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been processed, we download it as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the preprocessed data as a CSV file\n",
    "df[[\"review\", \"cleaned_review\", \"sentiment\"]].to_csv(\"IMDB Dataset Processed Lemma test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent the textual data as a Bag of Words or TF-IDF Vectors to be used when training the models.\n",
    "We split the data between the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representing the textual data in a suitable model (i.e. Bag of Words, TF-IDF Vectors)\n",
    "\n",
    "#Represent the text data using Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(df['cleaned_review'])\n",
    "\n",
    "#Alternatively, represent the text data using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['cleaned_review'])\n",
    "\n",
    "\n",
    "#Splitting the data into the training and test sets. Ensure that the train and test datasets are balanced by using stratify on the sentiments data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Labels (i.e. Sentiment)\n",
    "y = df['sentiment']\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y_encoded, test_size=0.2, random_state=42, stratify=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
