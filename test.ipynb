{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator, BranchPythonOperator\n",
    "from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\n",
    "from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateModelOperator, BigQueryExecuteQueryOperator\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from google.cloud import storage\n",
    "\n",
    "# DAG setup\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2023, 1, 1),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "dag = DAG(\n",
    "    'furniture_forecast_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='End-to-end pipeline for furniture demand forecasting with ARIMA_PLUS, Poisson regression, and automated retraining',\n",
    "    schedule_interval='@monthly',\n",
    ")\n",
    "\n",
    "# Task 1: Ingest Data\n",
    "def ingest_data():\n",
    "    response = requests.get(\"https://api.example.com/furniture_sales_data\")\n",
    "    data = response.json()\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(\"furniture-forecasting-bucket\")\n",
    "    bucket.blob(\"raw_data/furniture_sales_data.csv\").upload_from_string(df.to_csv(index=False), 'text/csv')\n",
    "\n",
    "ingest_task = PythonOperator(\n",
    "    task_id='ingest_data',\n",
    "    python_callable=ingest_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 2: Preprocess Data\n",
    "def preprocess_data():\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(\"furniture-forecasting-bucket\")\n",
    "    blob = bucket.blob(\"raw_data/furniture_sales_data.csv\")\n",
    "    df = pd.read_csv(blob.download_as_text())\n",
    "\n",
    "    df = df.dropna(subset=['sales'])\n",
    "    df['sales'] = df['sales'].fillna(method='ffill')\n",
    "\n",
    "    bucket.blob(\"preprocessed_data/preprocessed_sales_data.csv\").upload_from_string(df.to_csv(index=False), 'text/csv')\n",
    "\n",
    "preprocess_task = PythonOperator(\n",
    "    task_id='preprocess_data',\n",
    "    python_callable=preprocess_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 3: Load Data to BigQuery\n",
    "load_to_bigquery_task = GCSToBigQueryOperator(\n",
    "    task_id='load_data_to_bigquery',\n",
    "    bucket='furniture-forecasting-bucket',\n",
    "    source_objects=['preprocessed_data/preprocessed_sales_data.csv'],\n",
    "    destination_project_dataset_table='your_project.your_dataset.sales_data',\n",
    "    source_format='CSV',\n",
    "    skip_leading_rows=1,\n",
    "    write_disposition='WRITE_TRUNCATE',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 4: STL Decomposition\n",
    "def stl_decomposition():\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(\"furniture-forecasting-bucket\")\n",
    "    blob = bucket.blob(\"preprocessed_data/preprocessed_sales_data.csv\")\n",
    "    df = pd.read_csv(blob.download_as_text(), parse_dates=['date'])\n",
    "\n",
    "    stl = STL(df['sales'], seasonal=12)\n",
    "    result = stl.fit()\n",
    "    df['trend'] = result.trend\n",
    "    df['seasonal'] = result.seasonal\n",
    "    df['residual'] = result.resid\n",
    "\n",
    "    bucket.blob(\"decomposed_data/stl_decomposed_sales_data.csv\").upload_from_string(df.to_csv(index=False), 'text/csv')\n",
    "\n",
    "stl_decomposition_task = PythonOperator(\n",
    "    task_id='stl_decomposition',\n",
    "    python_callable=stl_decomposition,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 5a: Train ARIMA_PLUS Model on Seasonal Component\n",
    "train_arima_task = BigQueryCreateModelOperator(\n",
    "    task_id='train_arima_plus',\n",
    "    model_id='your_project.your_dataset.seasonal_forecast_model',\n",
    "    sql=\"\"\"\n",
    "        CREATE OR REPLACE MODEL `your_project.your_dataset.seasonal_forecast_model`\n",
    "        OPTIONS(\n",
    "            model_type = 'ARIMA_PLUS',\n",
    "            time_series_timestamp_col = 'date',\n",
    "            time_series_data_col = 'seasonal',\n",
    "            horizon = 12,\n",
    "            auto_arima = TRUE\n",
    "        ) AS\n",
    "        SELECT\n",
    "            date,\n",
    "            seasonal\n",
    "        FROM\n",
    "            `your_project.your_dataset.stl_decomposed_sales_data`\n",
    "        ORDER BY\n",
    "            date\n",
    "    \"\"\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 5b: Train Poisson Regression Model on Trend Component\n",
    "train_poisson_task = BigQueryCreateModelOperator(\n",
    "    task_id='train_poisson',\n",
    "    model_id='your_project.your_dataset.trend_forecast_model',\n",
    "    sql=\"\"\"\n",
    "        CREATE OR REPLACE MODEL `your_project.your_dataset.trend_forecast_model`\n",
    "        OPTIONS(\n",
    "            model_type = 'GLM',\n",
    "            model_registry = 'POISSON_REGRESSION',\n",
    "            time_series_timestamp_col = 'date',\n",
    "            time_series_data_col = 'trend'\n",
    "        ) AS\n",
    "        SELECT\n",
    "            date,\n",
    "            trend\n",
    "        FROM\n",
    "            `your_project.your_dataset.stl_decomposed_sales_data`\n",
    "        ORDER BY\n",
    "            date\n",
    "    \"\"\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 6a: Forecasting with ARIMA_PLUS for Seasonal Component\n",
    "forecast_arima_task = BigQueryExecuteQueryOperator(\n",
    "    task_id='forecast_arima_plus',\n",
    "    sql=\"\"\"\n",
    "        SELECT\n",
    "            forecast_timestamp,\n",
    "            forecast_value,\n",
    "            prediction_interval_lower_bound,\n",
    "            prediction_interval_upper_bound\n",
    "        FROM\n",
    "            ML.FORECAST(\n",
    "                MODEL `your_project.your_dataset.seasonal_forecast_model`,\n",
    "                STRUCT(12 AS horizon)\n",
    "            )\n",
    "    \"\"\",\n",
    "    destination_dataset_table='your_project.your_dataset.arima_plus_forecast',\n",
    "    write_disposition='WRITE_TRUNCATE',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 6b: Forecasting with Poisson Model for Trend Component\n",
    "forecast_poisson_task = BigQueryExecuteQueryOperator(\n",
    "    task_id='forecast_poisson',\n",
    "    sql=\"\"\"\n",
    "        SELECT\n",
    "            forecast_timestamp,\n",
    "            forecast_value\n",
    "        FROM\n",
    "            ML.FORECAST(\n",
    "                MODEL `your_project.your_dataset.trend_forecast_model`,\n",
    "                STRUCT(12 AS horizon)\n",
    "            )\n",
    "    \"\"\",\n",
    "    destination_dataset_table='your_project.your_dataset.poisson_forecast',\n",
    "    write_disposition='WRITE_TRUNCATE',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 7: Combine Forecasts\n",
    "def combine_forecasts():\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"\n",
    "        SELECT\n",
    "            a.forecast_timestamp,\n",
    "            a.forecast_value + b.forecast_value AS combined_forecast\n",
    "        FROM\n",
    "            `your_project.your_dataset.arima_plus_forecast` AS a\n",
    "        JOIN\n",
    "            `your_project.your_dataset.poisson_forecast` AS b\n",
    "        ON\n",
    "            a.forecast_timestamp = b.forecast_timestamp\n",
    "    \"\"\"\n",
    "    combined_forecast_df = client.query(query).to_dataframe()\n",
    "    combined_forecast_df.to_csv(\"gs://furniture-forecasting-bucket/predictions/combined_forecast.csv\", index=False)\n",
    "\n",
    "combine_forecast_task = PythonOperator(\n",
    "    task_id='combine_forecasts',\n",
    "    python_callable=combine_forecasts,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 8a: Evaluate ARIMA_PLUS Model\n",
    "evaluate_arima_task = BigQueryExecuteQueryOperator(\n",
    "    task_id='evaluate_arima_plus',\n",
    "    sql=\"\"\"\n",
    "        SELECT\n",
    "            *\n",
    "        FROM\n",
    "            ML.EVALUATE(\n",
    "                MODEL `your_project.your_dataset.seasonal_forecast_model`,\n",
    "                STRUCT(0.8 AS split_fraction)\n",
    "            )\n",
    "    \"\"\",\n",
    "    destination_dataset_table='your_project.your_dataset.arima_plus_eval_metrics',\n",
    "    write_disposition='WRITE_TRUNCATE',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 8b: Evaluate Poisson Model\n",
    "evaluate_poisson_task = BigQueryExecuteQueryOperator(\n",
    "    task_id='evaluate_poisson',\n",
    "    sql=\"\"\"\n",
    "        SELECT\n",
    "            *\n",
    "        FROM\n",
    "            ML.EVALUATE(\n",
    "                MODEL `your_project.your_dataset.trend_forecast_model`,\n",
    "                STRUCT(0.8 AS split_fraction)\n",
    "            )\n",
    "    \"\"\",\n",
    "    destination_dataset_table='your_project.your_dataset.poisson_eval_metrics',\n",
    "    write_disposition='WRITE_TRUNCATE',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 9: Check Retraining Condition\n",
    "def check_retraining_condition():\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"\n",
    "        SELECT mean_absolute_error\n",
    "        FROM `your_project.your_dataset.arima_plus_eval_metrics`\n",
    "        ORDER BY evaluation_date DESC\n",
    "        LIMIT 1\n",
    "    \"\"\"\n",
    "    results = client.query(query).result()\n",
    "    mae = next(results).mean_absolute_error\n",
    "\n",
    "    # Define your threshold for retraining\n",
    "    threshold = 10.0\n",
    "    if mae > threshold:\n",
    "        return 'retrain_model'\n",
    "    else:\n",
    "        return 'skip_retrain'\n",
    "\n",
    "check_retraining_task = BranchPythonOperator(\n",
    "    task_id='check_retraining_condition',\n",
    "    python_callable=check_retraining_condition,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 10a: Retrain ARIMA_PLUS Model (if required)\n",
    "retrain_arima_task = BigQueryCreateModelOperator(\n",
    "    task_id='retrain_arima_plus',\n",
    "    model_id='your_project.your_dataset.seasonal_forecast_model',\n",
    "    sql=\"\"\"\n",
    "        CREATE OR REPLACE MODEL `your_project.your_dataset.seasonal_forecast_model`\n",
    "        OPTIONS(\n",
    "            model_type = 'ARIMA_PLUS',\n",
    "            time_series_timestamp_col = 'date',\n",
    "            time_series_data_col = 'seasonal',\n",
    "            horizon = 12,\n",
    "            auto_arima = TRUE\n",
    "        ) AS\n",
    "        SELECT\n",
    "            date,\n",
    "            seasonal\n",
    "        FROM\n",
    "            `your_project.your_dataset.stl_decomposed_sales_data`\n",
    "        ORDER BY\n",
    "            date\n",
    "    \"\"\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 10b: Retrain Poisson Model (if required)\n",
    "retrain_poisson_task = BigQueryCreateModelOperator(\n",
    "    task_id='retrain_poisson',\n",
    "    model_id='your_project.your_dataset.trend_forecast_model',\n",
    "    sql=\"\"\"\n",
    "        CREATE OR REPLACE MODEL `your_project.your_dataset.trend_forecast_model`\n",
    "        OPTIONS(\n",
    "            model_type = 'GLM',\n",
    "            model_registry = 'POISSON_REGRESSION',\n",
    "            time_series_timestamp_col = 'date',\n",
    "            time_series_data_col = 'trend'\n",
    "        ) AS\n",
    "        SELECT\n",
    "            date,\n",
    "            trend\n",
    "        FROM\n",
    "            `your_project.your_dataset.stl_decomposed_sales_data`\n",
    "        ORDER BY\n",
    "            date\n",
    "    \"\"\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 11: Dummy Task to Skip Retraining\n",
    "skip_retrain_task = DummyOperator(task_id='skip_retrain', dag=dag)\n",
    "\n",
    "# DAG Dependencies\n",
    "ingest_task >> preprocess_task >> load_to_bigquery_task >> stl_decomposition_task\n",
    "stl_decomposition_task >> [train_arima_task, train_poisson_task]\n",
    "train_arima_task >> forecast_arima_task\n",
    "train_poisson_task >> forecast_poisson_task\n",
    "[forecast_arima_task, forecast_poisson_task] >> combine_forecast_task\n",
    "combine_forecast_task >> [evaluate_arima_task, evaluate_poisson_task]\n",
    "[evaluate_arima_task, evaluate_poisson_task] >> check_retraining_task\n",
    "check_retraining_task >> [retrain_arima_task, retrain_poisson_task, skip_retrain_task]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ie500",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
